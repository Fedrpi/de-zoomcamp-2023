{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m/Users/fdr/SqlProjects/Datatalks/data/csv/green_taxi_data_2023-01-20.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpython\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1234\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1233\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py:115\u001b[0m, in \u001b[0;36mPythonParser.__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_col_indices: \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    110\u001b[0m columns: \u001b[39mlist\u001b[39m[\u001b[39mlist\u001b[39m[Scalar \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m]]\n\u001b[1;32m    111\u001b[0m (\n\u001b[1;32m    112\u001b[0m     columns,\n\u001b[1;32m    113\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_original_columns,\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols,\n\u001b[0;32m--> 115\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_columns()\n\u001b[1;32m    117\u001b[0m \u001b[39m# Now self.columns has the set of columns that we will process.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m# The original set is stored in self.original_columns.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m# error: Cannot determine type of 'index_names'\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns: \u001b[39mlist\u001b[39m[Hashable]\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py:364\u001b[0m, in \u001b[0;36mPythonParser._infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mfor\u001b[39;00m level, hr \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(header):\n\u001b[1;32m    363\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m         line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffered_line()\n\u001b[1;32m    366\u001b[0m         \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_pos \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m hr:\n\u001b[1;32m    367\u001b[0m             line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_line()\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py:596\u001b[0m, in \u001b[0;36mPythonParser._buffered_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf[\u001b[39m0\u001b[39m]\n\u001b[1;32m    595\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_line()\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py:696\u001b[0m, in \u001b[0;36mPythonParser._next_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[1;32m    695\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m     orig_line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_iter_line(row_num\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    697\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m orig_line \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SqlProjects/Datatalks/env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py:760\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    758\u001b[0m     \u001b[39m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m    761\u001b[0m     \u001b[39m# for mypy\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(line, \u001b[39mlist\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[1;32m    323\u001b[0m     \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/fdr/SqlProjects/Datatalks/data/csv/green_taxi_data_2023-01-20.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/raw/csv/yellow_tripdata_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir('/Users/fdr/SqlProjects/Datatalks/data/raw/csv') if isfile(join('/Users/fdr/SqlProjects/Datatalks/data/raw/csv', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yellow_tripdata_2021.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv('data/raw/csv/yellow_tripdata_2021.csv', iterator=True, chunksize=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = next(df_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=0).to_sql(name='yellow_tripdata_trip', con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql(name='yellow_tripdata_trip', con=engine, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # user = params.user\n",
    "    # password = params.password\n",
    "    # host = params.host \n",
    "    # port = params.port \n",
    "    # db = params.db\n",
    "    # table_name = params.table_name\n",
    "    # url = params.url\n",
    "    # csv_name = 'output.csv'\n",
    "\n",
    "    # os.system(f\"wget {url} -O {csv_name}\")\n",
    "\n",
    "    # engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "\n",
    "    df_iter = pd.read_csv('data/raw/csv/yellow_tripdata_2021.csv', iterator=True, chunksize=100000)\n",
    "\n",
    "    df = next(df_iter)\n",
    "\n",
    "    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "    df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n",
    "\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='append')\n",
    "\n",
    "\n",
    "    while True: \n",
    "\n",
    "        try:\n",
    "            t_start = time()\n",
    "            \n",
    "            df = next(df_iter)\n",
    "\n",
    "            df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "            df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "\n",
    "            df.to_sql(name=table_name, con=engine, if_exists='append')\n",
    "\n",
    "            t_end = time()\n",
    "\n",
    "            print('inserted another chunk, took %.3f second' % (t_end - t_start))\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"Finished ingesting data into the postgres database\")\n",
    "            break\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='Ingest CSV data to Postgres')\n",
    "\n",
    "#     parser.add_argument('--user', required=True, help='user name for postgres')\n",
    "#     parser.add_argument('--password', required=True, help='password for postgres')\n",
    "#     parser.add_argument('--host', required=True, help='host for postgres')\n",
    "#     parser.add_argument('--port', required=True, help='port for postgres')\n",
    "#     parser.add_argument('--db', required=True, help='database name for postgres')\n",
    "#     parser.add_argument('--table_name', required=True, help='name of the table where we will write the results to')\n",
    "#     parser.add_argument('--url', required=True, help='url of the csv file')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, May 24 2022, 21:28:31) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8933d895466d1e6d819e95d3541c090661381f9dcd907533ea66bc312424ea5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
